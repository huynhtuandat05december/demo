{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Road Buddy - Traffic Video Question Answering\n",
    "\n",
    "This notebook runs the inference pipeline for traffic video question answering using the R-4B model.\n",
    "\n",
    "## Setup Instructions:\n",
    "1. Make sure GPU is enabled (Runtime → Change runtime type → GPU)\n",
    "2. Run all cells in order\n",
    "3. Upload your test data when prompted\n",
    "4. Download the results at the end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Check GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "else:\n",
    "    print(\"⚠️ GPU not available. Please enable GPU: Runtime → Change runtime type → GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q torch>=2.0.0 transformers>=4.40.0 accelerate>=0.20.0 \\\n",
    "    pillow>=10.0.0 opencv-python>=4.8.0 pandas>=2.0.0 tqdm>=4.65.0 numpy>=1.24.0\n",
    "\n",
    "print(\"✓ Dependencies installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Upload Project Files\n",
    "\n",
    "**Option A: Upload from GitHub (if you have a repo)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and modify if you want to clone from GitHub\n",
    "# !git clone https://github.com/YOUR_USERNAME/road_buddy.git\n",
    "# %cd road_buddy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Option B: Create project structure and upload files manually**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Create project structure\n",
    "!mkdir -p src\n",
    "!mkdir -p data/traffic_buddy_train+public_test/public_test/videos\n",
    "!mkdir -p output\n",
    "\n",
    "print(\"✓ Project structure created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create Source Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create __init__.py\n",
    "%%writefile src/__init__.py\n",
    "\"\"\"Road Buddy package.\"\"\"\n",
    "__version__ = \"0.1.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create config.py\n",
    "%%writefile src/config.py\n",
    "\"\"\"Configuration settings for the inference pipeline.\"\"\"\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Project paths\n",
    "PROJECT_ROOT = Path(\"/content\")\n",
    "DATA_DIR = PROJECT_ROOT / \"data\" / \"traffic_buddy_train+public_test\"\n",
    "PUBLIC_TEST_DIR = DATA_DIR / \"public_test\"\n",
    "PUBLIC_TEST_JSON = PUBLIC_TEST_DIR / \"public_test.json\"\n",
    "VIDEOS_DIR = PUBLIC_TEST_DIR / \"videos\"\n",
    "OUTPUT_DIR = PROJECT_ROOT / \"output\"\n",
    "SUBMISSION_FILE = OUTPUT_DIR / \"submission.csv\"\n",
    "\n",
    "# Model configuration\n",
    "MODEL_NAME = \"YannQi/R-4B\"\n",
    "DEVICE = \"cuda\"  # or \"cpu\"\n",
    "TRUST_REMOTE_CODE = True\n",
    "\n",
    "# Video processing configuration\n",
    "FRAME_SAMPLE_RATE = 1.0  # Extract 1 frame per second\n",
    "MAX_FRAMES_PER_VIDEO = 20  # Maximum number of frames to extract\n",
    "USE_MID_FRAME_ONLY = False  # If True, only extract the middle frame\n",
    "\n",
    "# Inference configuration\n",
    "THINKING_MODE = \"auto\"  # \"auto\", \"explicit\", or \"non-thinking\"\n",
    "MAX_NEW_TOKENS = 512\n",
    "TEMPERATURE = 0.1  # Low temperature for more deterministic outputs\n",
    "DO_SAMPLE = False  # Use greedy decoding for consistent answers\n",
    "\n",
    "# Batch processing\n",
    "BATCH_SIZE = 1  # Process one question at a time (model handles one image-question pair)\n",
    "CACHE_FRAMES = True  # Cache extracted frames when multiple questions use same video\n",
    "\n",
    "# Prompt template\n",
    "PROMPT_TEMPLATE = \"\"\"Dựa vào video này, hãy trả lời câu hỏi sau:\n",
    "\n",
    "Câu hỏi: {question}\n",
    "\n",
    "Các lựa chọn:\n",
    "{choices}\n",
    "\n",
    "Hãy chọn đáp án đúng (chỉ trả lời A, B, C, hoặc D).\"\"\"\n",
    "\n",
    "# Answer parsing\n",
    "VALID_ANSWERS = [\"A\", \"B\", \"C\", \"D\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create video_processor.py\n",
    "%%writefile src/video_processor.py\n",
    "\"\"\"Video processing utilities for extracting frames.\"\"\"\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import List, Optional\n",
    "from PIL import Image\n",
    "\n",
    "from src import config\n",
    "\n",
    "\n",
    "class VideoProcessor:\n",
    "    \"\"\"Handles video frame extraction and preprocessing.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        frame_sample_rate: float = config.FRAME_SAMPLE_RATE,\n",
    "        max_frames: int = config.MAX_FRAMES_PER_VIDEO,\n",
    "        use_mid_frame_only: bool = config.USE_MID_FRAME_ONLY,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize video processor.\n",
    "\n",
    "        Args:\n",
    "            frame_sample_rate: Frames to extract per second\n",
    "            max_frames: Maximum number of frames to extract\n",
    "            use_mid_frame_only: If True, only extract the middle frame\n",
    "        \"\"\"\n",
    "        self.frame_sample_rate = frame_sample_rate\n",
    "        self.max_frames = max_frames\n",
    "        self.use_mid_frame_only = use_mid_frame_only\n",
    "        self._frame_cache = {}\n",
    "\n",
    "    def extract_frames(self, video_path: Path) -> List[Image.Image]:\n",
    "        \"\"\"\n",
    "        Extract frames from a video file.\n",
    "\n",
    "        Args:\n",
    "            video_path: Path to the video file\n",
    "\n",
    "        Returns:\n",
    "            List of PIL Images extracted from the video\n",
    "        \"\"\"\n",
    "        # Check cache if enabled\n",
    "        if config.CACHE_FRAMES and str(video_path) in self._frame_cache:\n",
    "            return self._frame_cache[str(video_path)]\n",
    "\n",
    "        if not video_path.exists():\n",
    "            raise FileNotFoundError(f\"Video file not found: {video_path}\")\n",
    "\n",
    "        # Open video\n",
    "        cap = cv2.VideoCapture(str(video_path))\n",
    "        if not cap.isOpened():\n",
    "            raise ValueError(f\"Failed to open video: {video_path}\")\n",
    "\n",
    "        try:\n",
    "            # Get video properties\n",
    "            fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "            duration = total_frames / fps if fps > 0 else 0\n",
    "\n",
    "            frames = []\n",
    "\n",
    "            if self.use_mid_frame_only:\n",
    "                # Extract only the middle frame\n",
    "                mid_frame_idx = total_frames // 2\n",
    "                cap.set(cv2.CAP_PROP_POS_FRAMES, mid_frame_idx)\n",
    "                ret, frame = cap.read()\n",
    "                if ret:\n",
    "                    frames.append(self._convert_frame(frame))\n",
    "            else:\n",
    "                # Calculate frame sampling interval\n",
    "                frame_interval = int(fps / self.frame_sample_rate) if self.frame_sample_rate > 0 else 1\n",
    "                frame_interval = max(1, frame_interval)\n",
    "\n",
    "                frame_idx = 0\n",
    "                while len(frames) < self.max_frames:\n",
    "                    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
    "                    ret, frame = cap.read()\n",
    "\n",
    "                    if not ret:\n",
    "                        break\n",
    "\n",
    "                    frames.append(self._convert_frame(frame))\n",
    "                    frame_idx += frame_interval\n",
    "\n",
    "                    if frame_idx >= total_frames:\n",
    "                        break\n",
    "\n",
    "            # Cache frames if enabled\n",
    "            if config.CACHE_FRAMES:\n",
    "                self._frame_cache[str(video_path)] = frames\n",
    "\n",
    "            return frames\n",
    "\n",
    "        finally:\n",
    "            cap.release()\n",
    "\n",
    "    def _convert_frame(self, frame: np.ndarray) -> Image.Image:\n",
    "        \"\"\"\n",
    "        Convert OpenCV frame (BGR) to PIL Image (RGB).\n",
    "\n",
    "        Args:\n",
    "            frame: OpenCV frame in BGR format\n",
    "\n",
    "        Returns:\n",
    "            PIL Image in RGB format\n",
    "        \"\"\"\n",
    "        # Convert BGR to RGB\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        return Image.fromarray(frame_rgb)\n",
    "\n",
    "    def clear_cache(self):\n",
    "        \"\"\"Clear the frame cache.\"\"\"\n",
    "        self._frame_cache.clear()\n",
    "\n",
    "    def get_cache_size(self) -> int:\n",
    "        \"\"\"Get the number of cached videos.\"\"\"\n",
    "        return len(self._frame_cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create inference.py\n",
    "%%writefile src/inference.py\n",
    "\"\"\"Main inference pipeline for traffic video question answering.\"\"\"\n",
    "\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from transformers import AutoModel, AutoProcessor\n",
    "\n",
    "from src import config\n",
    "from src.video_processor import VideoProcessor\n",
    "\n",
    "\n",
    "class R4BInferencePipeline:\n",
    "    \"\"\"Inference pipeline using R-4B model for video question answering.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = config.MODEL_NAME,\n",
    "        device: str = config.DEVICE,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the inference pipeline.\n",
    "\n",
    "        Args:\n",
    "            model_name: HuggingFace model name\n",
    "            device: Device to run inference on ('cuda' or 'cpu')\n",
    "        \"\"\"\n",
    "        self.device = device\n",
    "        self.model_name = model_name\n",
    "\n",
    "        print(f\"Loading model: {model_name}\")\n",
    "        print(f\"Device: {device}\")\n",
    "\n",
    "        # Load model and processor\n",
    "        self.model = AutoModel.from_pretrained(\n",
    "            model_name,\n",
    "            trust_remote_code=config.TRUST_REMOTE_CODE,\n",
    "            torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
    "        ).to(device)\n",
    "        self.model.eval()\n",
    "\n",
    "        self.processor = AutoProcessor.from_pretrained(\n",
    "            model_name,\n",
    "            trust_remote_code=config.TRUST_REMOTE_CODE,\n",
    "        )\n",
    "\n",
    "        # Initialize video processor\n",
    "        self.video_processor = VideoProcessor()\n",
    "\n",
    "        print(\"Model and processor loaded successfully!\")\n",
    "\n",
    "    def format_prompt(self, question: str, choices: List[str]) -> str:\n",
    "        \"\"\"\n",
    "        Format the prompt for the model.\n",
    "\n",
    "        Args:\n",
    "            question: The question to ask\n",
    "            choices: List of answer choices\n",
    "\n",
    "        Returns:\n",
    "            Formatted prompt string\n",
    "        \"\"\"\n",
    "        choices_str = \"\\n\".join(choices)\n",
    "        return config.PROMPT_TEMPLATE.format(\n",
    "            question=question,\n",
    "            choices=choices_str,\n",
    "        )\n",
    "\n",
    "    def parse_answer(self, response: str) -> str:\n",
    "        \"\"\"\n",
    "        Parse the answer from model response.\n",
    "\n",
    "        Args:\n",
    "            response: Raw model output\n",
    "\n",
    "        Returns:\n",
    "            Parsed answer (A, B, C, or D)\n",
    "        \"\"\"\n",
    "        # Try to find answer pattern like \"A\", \"B.\", \"Answer: C\", etc.\n",
    "        patterns = [\n",
    "            r'\\b([ABCD])\\b',  # Single letter\n",
    "            r'(?:đáp án|answer|chọn)[\\s:]*([ABCD])',  # \"đáp án A\" or \"answer: B\"\n",
    "            r'^([ABCD])[.\\s]',  # \"A. \" at start\n",
    "        ]\n",
    "\n",
    "        for pattern in patterns:\n",
    "            matches = re.findall(pattern, response, re.IGNORECASE)\n",
    "            if matches:\n",
    "                answer = matches[0].upper()\n",
    "                if answer in config.VALID_ANSWERS:\n",
    "                    return answer\n",
    "\n",
    "        # If no clear answer found, try to extract the first valid letter\n",
    "        for char in response.upper():\n",
    "            if char in config.VALID_ANSWERS:\n",
    "                return char\n",
    "\n",
    "        # Default to A if no answer found\n",
    "        print(f\"Warning: Could not parse answer from response: {response[:100]}\")\n",
    "        return \"A\"\n",
    "\n",
    "    def run_inference(self, question_data: Dict) -> str:\n",
    "        \"\"\"\n",
    "        Run inference for a single question.\n",
    "\n",
    "        Args:\n",
    "            question_data: Dictionary containing question information\n",
    "\n",
    "        Returns:\n",
    "            Predicted answer (A, B, C, or D)\n",
    "        \"\"\"\n",
    "        # Get video path\n",
    "        video_path = config.DATA_DIR / question_data[\"video_path\"]\n",
    "\n",
    "        # Extract frames from video\n",
    "        try:\n",
    "            frames = self.video_processor.extract_frames(video_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting frames from {video_path}: {e}\")\n",
    "            return \"A\"  # Default answer on error\n",
    "\n",
    "        if not frames:\n",
    "            print(f\"Warning: No frames extracted from {video_path}\")\n",
    "            return \"A\"\n",
    "\n",
    "        # Use the middle frame (or first frame if only one)\n",
    "        frame = frames[len(frames) // 2]\n",
    "\n",
    "        # Format prompt\n",
    "        prompt = self.format_prompt(\n",
    "            question=question_data[\"question\"],\n",
    "            choices=question_data[\"choices\"],\n",
    "        )\n",
    "\n",
    "        # Prepare inputs\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"image\", \"image\": frame},\n",
    "                    {\"type\": \"text\", \"text\": prompt},\n",
    "                ],\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        # Process inputs\n",
    "        text = self.processor.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True,\n",
    "        )\n",
    "        inputs = self.processor(\n",
    "            text=[text],\n",
    "            images=[frame],\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "        )\n",
    "\n",
    "        # Move inputs to device\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "\n",
    "        # Generate response\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=config.MAX_NEW_TOKENS,\n",
    "                do_sample=config.DO_SAMPLE,\n",
    "                temperature=config.TEMPERATURE if config.DO_SAMPLE else None,\n",
    "                thinking_mode=config.THINKING_MODE,\n",
    "            )\n",
    "\n",
    "        # Decode response\n",
    "        response = self.processor.batch_decode(\n",
    "            outputs,\n",
    "            skip_special_tokens=True,\n",
    "            clean_up_tokenization_spaces=True,\n",
    "        )[0]\n",
    "\n",
    "        # Parse answer\n",
    "        answer = self.parse_answer(response)\n",
    "\n",
    "        return answer\n",
    "\n",
    "    def run_pipeline(self, test_json_path: Path, output_csv_path: Path):\n",
    "        \"\"\"\n",
    "        Run the full inference pipeline on test data.\n",
    "\n",
    "        Args:\n",
    "            test_json_path: Path to test JSON file\n",
    "            output_csv_path: Path to save submission CSV\n",
    "        \"\"\"\n",
    "        # Load test data\n",
    "        print(f\"Loading test data from: {test_json_path}\")\n",
    "        with open(test_json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            test_data = json.load(f)\n",
    "\n",
    "        questions = test_data[\"data\"]\n",
    "        print(f\"Total questions: {len(questions)}\")\n",
    "\n",
    "        # Run inference\n",
    "        results = []\n",
    "        for question_data in tqdm(questions, desc=\"Processing questions\"):\n",
    "            question_id = question_data[\"id\"]\n",
    "            answer = self.run_inference(question_data)\n",
    "            results.append({\"id\": question_id, \"answer\": answer})\n",
    "\n",
    "        # Save results\n",
    "        df = pd.DataFrame(results)\n",
    "        output_csv_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        df.to_csv(output_csv_path, index=False)\n",
    "        print(f\"\\nResults saved to: {output_csv_path}\")\n",
    "        print(f\"Total predictions: {len(results)}\")\n",
    "\n",
    "        # Clear cache\n",
    "        self.video_processor.clear_cache()\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main entry point for the inference pipeline.\"\"\"\n",
    "    # Initialize pipeline\n",
    "    pipeline = R4BInferencePipeline()\n",
    "\n",
    "    # Run inference on public test data\n",
    "    pipeline.run_pipeline(\n",
    "        test_json_path=config.PUBLIC_TEST_JSON,\n",
    "        output_csv_path=config.SUBMISSION_FILE,\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Upload Test Data\n",
    "\n",
    "You need to upload:\n",
    "1. `public_test.json` - The test questions file\n",
    "2. Video files - All the video files referenced in the JSON\n",
    "\n",
    "**Option A: Upload from Google Drive**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Modify this path to point to your data in Google Drive\n",
    "# !cp -r \"/content/drive/MyDrive/your_data_folder/*\" /content/data/traffic_buddy_train+public_test/public_test/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Option B: Upload directly (for small datasets)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import shutil\n",
    "\n",
    "# Upload public_test.json\n",
    "print(\"Please upload public_test.json\")\n",
    "uploaded = files.upload()\n",
    "for filename in uploaded.keys():\n",
    "    shutil.move(filename, \"/content/data/traffic_buddy_train+public_test/public_test/public_test.json\")\n",
    "    print(f\"✓ {filename} uploaded successfully\")\n",
    "\n",
    "# Upload videos\n",
    "print(\"\\nPlease upload video files (you may need to zip them first)\")\n",
    "uploaded = files.upload()\n",
    "for filename in uploaded.keys():\n",
    "    if filename.endswith('.zip'):\n",
    "        !unzip -q {filename} -d /content/data/traffic_buddy_train+public_test/public_test/videos/\n",
    "        print(f\"✓ {filename} extracted\")\n",
    "    else:\n",
    "        shutil.move(filename, f\"/content/data/traffic_buddy_train+public_test/public_test/videos/{filename}\")\n",
    "        print(f\"✓ {filename} uploaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Option C: Download from URL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and modify if you have a download link\n",
    "# !wget -O /content/data.zip \"YOUR_DOWNLOAD_LINK\"\n",
    "# !unzip -q /content/data.zip -d /content/data/traffic_buddy_train+public_test/public_test/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Verify Data Upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Check if JSON file exists\n",
    "json_path = Path(\"/content/data/traffic_buddy_train+public_test/public_test/public_test.json\")\n",
    "if json_path.exists():\n",
    "    with open(json_path) as f:\n",
    "        data = json.load(f)\n",
    "    print(f\"✓ JSON file found: {len(data['data'])} questions\")\n",
    "else:\n",
    "    print(\"✗ JSON file not found!\")\n",
    "\n",
    "# Check video files\n",
    "videos_dir = Path(\"/content/data/traffic_buddy_train+public_test/public_test/videos\")\n",
    "if videos_dir.exists():\n",
    "    video_files = list(videos_dir.glob(\"*.mp4\")) + list(videos_dir.glob(\"*.avi\"))\n",
    "    print(f\"✓ Found {len(video_files)} video files\")\n",
    "else:\n",
    "    print(\"✗ Videos directory not found!\")\n",
    "\n",
    "# List directory structure\n",
    "!ls -lh /content/data/traffic_buddy_train+public_test/public_test/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Run Inference Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/content')\n",
    "\n",
    "from src.inference import main\n",
    "\n",
    "# Run the pipeline\n",
    "print(\"Starting inference pipeline...\\n\")\n",
    "main()\n",
    "print(\"\\n✓ Inference completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. View Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load and display results\n",
    "results_df = pd.read_csv(\"/content/output/submission.csv\")\n",
    "print(f\"Total predictions: {len(results_df)}\")\n",
    "print(f\"\\nFirst 10 predictions:\")\n",
    "print(results_df.head(10))\n",
    "\n",
    "# Show answer distribution\n",
    "print(f\"\\nAnswer distribution:\")\n",
    "print(results_df['answer'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Download Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "# Download the submission file\n",
    "files.download(\"/content/output/submission.csv\")\n",
    "print(\"✓ Submission file downloaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. (Optional) Test on a Single Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on a single question to debug\n",
    "from src.inference import R4BInferencePipeline\n",
    "import json\n",
    "\n",
    "# Load test data\n",
    "with open(\"/content/data/traffic_buddy_train+public_test/public_test/public_test.json\") as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "# Initialize pipeline\n",
    "pipeline = R4BInferencePipeline()\n",
    "\n",
    "# Test first question\n",
    "first_question = test_data[\"data\"][0]\n",
    "print(f\"Question: {first_question['question']}\")\n",
    "print(f\"Choices: {first_question['choices']}\")\n",
    "\n",
    "answer = pipeline.run_inference(first_question)\n",
    "print(f\"\\nPredicted answer: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Troubleshooting\n",
    "\n",
    "### Out of Memory Error\n",
    "If you get OOM errors, try:\n",
    "- Reducing `MAX_FRAMES_PER_VIDEO` in config\n",
    "- Setting `USE_MID_FRAME_ONLY = True` to use only one frame per video\n",
    "- Using a smaller batch size\n",
    "\n",
    "### Video Loading Errors\n",
    "- Make sure video paths in the JSON match the actual file names\n",
    "- Check that all videos are in the correct directory\n",
    "- Verify video files are not corrupted\n",
    "\n",
    "### Model Loading Errors\n",
    "- Ensure you have a stable internet connection\n",
    "- The model will be downloaded from HuggingFace on first run\n",
    "- May require HuggingFace authentication for private models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
